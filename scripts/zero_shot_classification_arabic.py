from transformers import pipeline
import pandas as pd
import os
import re
from tqdm import tqdm
from datetime import datetime
import torch

class AdvancedArabicLegalClassifier:
    def __init__(self):
        # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        self.models = {
            "mDeBERTa-Legal": "MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
            "Legal-Arabic-BERT": "aubmindlab/bert-base-arabertv02",
            "XLM-R-Legal": "joeddav/xlm-roberta-large-xnli"
        }
        
        # Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù…Ø¹ Ø£Ù…Ø«Ù„Ø©
        self.labels = [
            "Ù†Ø¸Ø§Ù… Ø£Ùˆ Ù„Ø§Ø¦Ø­Ø©",      # Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„ØªØ´Ø±ÙŠØ¹ÙŠØ© Ø§Ù„Ø±Ø³Ù…ÙŠØ©
            "ÙˆØ«ÙŠÙ‚Ø© ØªÙ†Ø¸ÙŠÙ…ÙŠØ©",      # Ù„Ù„Ø§Ø´ØªØ±Ø§Ø·Ø§Øª ÙˆØ§Ù„Ø¶ÙˆØ§Ø¨Ø·
            "Ù‚Ø±Ø§Ø± Ø¥Ø¯Ø§Ø±ÙŠ",         # Ù„Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø³Ù…ÙŠØ©
            "Ù…Ø³ØªÙ†Ø¯ ØªØ¹Ø§Ù‚Ø¯ÙŠ",       # Ù„Ù„Ø¹Ù‚ÙˆØ¯ ÙˆØ§Ù„Ø§ØªÙØ§Ù‚ÙŠØ§Øª
            "ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø¶Ø§Ø¦ÙŠØ©"        # Ù„Ù„Ø£Ø­ÙƒØ§Ù… ÙˆØ§Ù„ÙØªØ§ÙˆÙ‰
        ]
        
        # Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø²Ø²Ø© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†
        self.enhanced_rules = [
            (r'(Ù†Ø¸Ø§Ù…|Ù„Ø§Ø¦Ø­Ø©|Ø§Ù„Ù„Ø§Ø¦Ø­Ø©)\s', ("Ù†Ø¸Ø§Ù… Ø£Ùˆ Ù„Ø§Ø¦Ø­Ø©", 0.95)),
            (r'(Ø§Ø´ØªØ±Ø§Ø·Ø§Øª|Ø¶ÙˆØ§Ø¨Ø·|Ù…Ø¹Ø§ÙŠÙŠØ±)\s', ("ÙˆØ«ÙŠÙ‚Ø© ØªÙ†Ø¸ÙŠÙ…ÙŠØ©", 0.9)),
            (r'(Ù‚Ø±Ø§Ø±|ØªØ¹Ù…ÙŠÙ…|ØªØ¹Ù„ÙŠÙ…Ø§Øª)\s(ÙˆØ²Ø§Ø±ÙŠ|Ø¥Ø¯Ø§Ø±ÙŠ)', ("Ù‚Ø±Ø§Ø± Ø¥Ø¯Ø§Ø±ÙŠ", 0.92)),
            (r'(Ø¹Ù‚Ø¯|Ø§ØªÙØ§Ù‚ÙŠØ©|Ù…Ø°ÙƒØ±Ø©\sØªÙØ§Ù‡Ù…)\s', ("Ù…Ø³ØªÙ†Ø¯ ØªØ¹Ø§Ù‚Ø¯ÙŠ", 0.88)),
            (r'(Ø­ÙƒÙ…|Ù‚Ø±Ø§Ø±\sÙ‚Ø¶Ø§Ø¦ÙŠ|ÙØªÙˆÙ‰)\s', ("ÙˆØ«ÙŠÙ‚Ø© Ù‚Ø¶Ø§Ø¦ÙŠØ©", 0.91)),
            (r'ØµØ­ÙŠÙØ©\sØ£Ù…\sØ§Ù„Ù‚Ø±Ù‰', ("Ù†Ø¸Ø§Ù… Ø£Ùˆ Ù„Ø§Ø¦Ø­Ø©", 0.97))
        ]
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØµÙ†ÙŠÙ
        self.template = "Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© ØªØµÙ†Ù Ø¶Ù…Ù† ÙØ¦Ø© {}."
        self.min_confidence = 0.65
        self.results = []
        self.metadata_patterns = {
            'Ø±Ù‚Ù…_Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©': r'Ø±Ù‚Ù…\sØ§Ù„ÙˆØ«ÙŠÙ‚Ø©\s*:\s*(\d+)',
            'ØªØ§Ø±ÙŠØ®_Ø§Ù„Ø¥ØµØ¯Ø§Ø±': r'ØªØ§Ø±ÙŠØ®\sØ§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯\s*:\s*([\d/]+)',
            'Ø¬Ù‡Ø©_Ø§Ù„Ø¥ØµØ¯Ø§Ø±': r'Ø¬Ù‡Ø©\sØ§Ù„Ø¥ØµØ¯Ø§Ø±\s*\n-+\s*\n(.+)'
        }

    def extract_metadata(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù…Ù† Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù†Ø¸Ù…"""
        metadata = {}
        for field, pattern in self.metadata_patterns.items():
            match = re.search(pattern, text)
            if match:
                metadata[field] = match.group(1).strip()
        return metadata

    def preprocess_legal_text(self, text):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙ‚Ø¯Ù…Ø© Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        # Ø§Ù„ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù„ØºÙˆÙŠ
        replacements = {
            'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ù€': '', 'Ø©': 'Ù‡',
            'Ù‰': 'ÙŠ', 'Ø¦': 'Ø¡', 'Ø¤': 'Ø¡', 'Ù„Ø§': 'Ù„Ø§'
        }
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…
        text = re.sub(r'[\u064b-\u065f]', '', text)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ´ÙƒÙŠÙ„
        text = re.sub(r'[Ù€ØŒØ›:\-\*_]', ' ', text)     # Ø¥Ø²Ø§Ù„Ø© Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ø§Ù„Ù…Ø²Ø¹Ø¬Ø©
        text = re.sub(r'\s+', ' ', text).strip()      # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        
        return text

    def detect_document_structure(self, text):
        """Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ Ù„Ù„ÙˆØ«ÙŠÙ‚Ø©"""
        structure_markers = [
            'Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©', 'Ø¬Ù‡Ø© Ø§Ù„Ø¥ØµØ¯Ø§Ø±', 
            'ÙØ¦Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©', 'Ù†Ø¨Ø°Ø© Ø§Ù„ÙˆØ«ÙŠÙ‚Ø©',
            'ØµØ­ÙŠÙØ© Ø£Ù… Ø§Ù„Ù‚Ø±Ù‰'
        ]
        return sum(marker in text for marker in structure_markers) >= 3

    def apply_enhanced_rules(self, text):
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø²Ø²Ø© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡ÙŠÙƒÙ„"""
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø±Ø³Ù…ÙŠ Ø£ÙˆÙ„Ø§Ù‹
        if self.detect_document_structure(text):
            metadata = self.extract_metadata(text)
            if metadata.get('Ø¬Ù‡Ø©_Ø§Ù„Ø¥ØµØ¯Ø§Ø±'):
                if 'Ù„Ø§Ø¦Ø­Ø©' in text[:100] or 'Ù†Ø¸Ø§Ù…' in text[:100]:
                    return "Ù†Ø¸Ø§Ù… Ø£Ùˆ Ù„Ø§Ø¦Ø­Ø©", 0.97
                if 'Ù‚Ø±Ø§Ø±' in text[:100]:
                    return "Ù‚Ø±Ø§Ø± Ø¥Ø¯Ø§Ø±ÙŠ", 0.95
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
        for pattern, (label, confidence) in self.enhanced_rules:
            if re.search(pattern, text, re.IGNORECASE):
                return label, confidence
        return None, 0

    def classify_with_ai(self, text, model_name):
        """Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª"""
        try:
            classifier = pipeline(
                "zero-shot-classification",
                model=self.models[model_name],
                device=0 if torch.cuda.is_available() else -1,
                batch_size=4 if torch.cuda.is_available() else 1
            )
            
            # Ø§Ù„ØªØµÙ†ÙŠÙ Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³ÙŠØ§Ù‚
            chunks = self.smart_chunking(text)
            predictions = []
            
            for chunk in chunks:
                result = classifier(
                    chunk,
                    candidate_labels=self.labels,
                    hypothesis_template=self.template,
                    multi_label=False
                )
                
                # ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø«Ù‚Ø© Ù„Ù„Ù‚Ø·Ø¹ Ø§Ù„Ø­Ø§Ø³Ù…Ø©
                if any(kw in chunk for kw in ['Ø§Ù„Ù…Ø§Ø¯Ø©', 'ÙŠÙ†Øµ', 'ÙŠÙ‚Ø±Ø±']):
                    result['scores'][0] = min(result['scores'][0] * 1.2, 1.0)
                
                predictions.append((result['labels'][0], result['scores'][0]))
            
            # Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
            if not predictions:
                return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯", 0.0
                
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­
            label_scores = {}
            for label, score in predictions:
                if label in label_scores:
                    label_scores[label].append(score)
                else:
                    label_scores[label] = [score]
            
            avg_scores = {
                label: sum(scores)/len(scores)
                for label, scores in label_scores.items()
            }
            
            best_label = max(avg_scores.items(), key=lambda x: x[1])
            return best_label[0], best_label[1]
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµÙ†ÙŠÙ ({model_name}): {str(e)}")
            return "error", 0.0

    def smart_chunking(self, text, max_words=350):
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø°ÙƒÙŠØ© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù†Ù‰ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ"""
        # ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ø£ÙˆÙ„Ø§Ù‹
        paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
        chunks = []
        current_chunk = []
        current_length = 0
        
        for para in paragraphs:
            words = para.split()
            para_length = len(words)
            
            if para_length > max_words:
                # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ÙÙ‚Ø±Ø© Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ø§Ù‹ØŒ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø¬Ù…Ù„
                sentences = [s.strip() for s in re.split(r'[\.\ØŸ\!]', para) if s.strip()]
                for sent in sentences:
                    sent_words = sent.split()
                    if current_length + len(sent_words) <= max_words:
                        current_chunk.append(sent)
                        current_length += len(sent_words)
                    else:
                        if current_chunk:
                            chunks.append(' '.join(current_chunk))
                        current_chunk = [sent]
                        current_length = len(sent_words)
            else:
                if current_length + para_length <= max_words:
                    current_chunk.append(para)
                    current_length += para_length
                else:
                    if current_chunk:
                        chunks.append(' '.join(current_chunk))
                    current_chunk = [para]
                    current_length = para_length
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks

    def process_document(self, file_path):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªÙƒØ§Ù…Ù„Ø© Ù„Ù„ÙˆØ«ÙŠÙ‚Ø© Ø§Ù„ÙˆØ§Ø­Ø¯Ø©"""
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
        
        # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©
        text = self.preprocess_legal_text(text)
        file_name = os.path.basename(file_path)
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø§Ù„Ù…Ø¹Ø²Ø²
        rule_label, rule_confidence = self.apply_enhanced_rules(text)
        if rule_label and rule_confidence >= 0.9:
            return file_name, rule_label, rule_confidence
        
        # Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
        model_results = []
        for model_name in self.models:
            label, confidence = self.classify_with_ai(text, model_name)
            if confidence >= self.min_confidence:
                model_results.append((label, confidence, model_name))
        
        if not model_results:
            return file_name, "ØºÙŠØ± Ù…Ø­Ø¯Ø¯", 0.0
            
        # Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„ØªÙˆØ§ÙÙ‚
        best_result = max(model_results, key=lambda x: x[1])
        return file_name, best_result[0], best_result[1]

    def process_folder(self, input_folder, output_file):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"""
        if not os.path.exists(input_folder):
            raise FileNotFoundError(f"Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø­Ø¯Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {input_folder}")
        
        files = [f for f in os.listdir(input_folder) if f.endswith(".txt")]
        print(f" was found {len(files)} legal documenst")
        
        for file_name in tqdm(files, desc="ğŸ” Ø¬Ø§Ø±ÙŠ ØªØµÙ†ÙŠÙ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚"):
            file_path = os.path.join(input_folder, file_name)
            
            try:
                doc_name, label, confidence = self.process_document(file_path)
                
                self.results.append({
                    "Ø§Ø³Ù…_Ø§Ù„Ù…Ù„Ù": doc_name,
                    "Ø§Ù„ØªØµÙ†ÙŠÙ": label,
                    "Ø«Ù‚Ø©_Ø§Ù„ØªØµÙ†ÙŠÙ": round(confidence, 3),
                    "ÙˆÙ‚Øª_Ø§Ù„ØªØµÙ†ÙŠÙ": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
                
                print(f"âœ… {doc_name[:25]}... | {label:15} | {confidence:.2f}")
            
            except Exception as e:
                print(f" Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {file_name}: {str(e)}")
                continue
        
        self.save_results(output_file)
        self.generate_detailed_report()

    def save_results(self, output_file):
        """Save results with additional data"""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        df = pd.DataFrame(self.results)
        
        # ØªØ­Ø³ÙŠÙ† ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        df = df.sort_values(by=['Ø«Ù‚Ø©_Ø§Ù„ØªØµÙ†ÙŠÙ'], ascending=False)
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\n Results saved in: {output_file}")

    def generate_detailed_report(self):
        """Create a detailed report with analysis"""
        if not self.results:
            print(" Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±")
            return
        
        df = pd.DataFrame(self.results)
        
        print("\n Advanced Analytical Report:")
        print(f"\n Total number {len(df)} documents")
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹
        print("\n Categories distribution:")
        dist = df['Ø§Ù„ØªØµÙ†ÙŠÙ'].value_counts()
        print(dist.to_string())
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©
        print("\n Average confidence by classification:")
        conf_table = df.groupby('Ø§Ù„ØªØµÙ†ÙŠÙ')['Ø«Ù‚Ø©_Ø§Ù„ØªØµÙ†ÙŠÙ'].agg(['mean', 'count'])
        print(conf_table.round(2).to_string())
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ù†Ø®ÙØ¶Ø© Ø§Ù„Ø«Ù‚Ø©
        low_conf = df[df['Ø«Ù‚Ø©_Ø§Ù„ØªØµÙ†ÙŠÙ'] < 0.7]
        if not low_conf.empty:
            print("\n Files with low confidence (<0.7):")
            print(low_conf[['Ø§Ø³Ù…_Ø§Ù„Ù…Ù„Ù', 'Ø§Ù„ØªØµÙ†ÙŠÙ', 'Ø«Ù‚Ø©_Ø§Ù„ØªØµÙ†ÙŠÙ']].to_string(index=False))

if __name__ == "__main__":
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
    INPUT_DIR = "../data/txt_files"
    OUTPUT_PATH = "../outputs/classified_docs_enhanced.csv"
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ø¸Ø§Ù…
    print("\n Advanced Classification System for Arabic Legal Documents")
    print("----------------------------------------")
    
    classifier = AdvancedArabicLegalClassifier()
    try:
        classifier.process_folder(INPUT_DIR, OUTPUT_PATH)
        print("\n Classification completed successfully!")
    except Exception as e:
        print(f"\n an error occured {str(e)}")